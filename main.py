#!/usr/bin/env python3
"""
Gemini PDF Indexer - ä½¿ç”¨ Gemini 2.5 Flash è‡ªåŠ¨æå–PDFæ ‡é¢˜å¹¶åˆ›å»ºç›®å½•

ä¸»è¦åŠŸèƒ½ï¼š
1. è‡ªåŠ¨åˆ†æPDFæ–‡æ¡£ç»“æ„
2. ä½¿ç”¨Gemini AIæå–æ ‡é¢˜å±‚çº§
3. å°†ç›®å½•ä¿¡æ¯å†™å›PDFæ–‡ä»¶
4. æ”¯æŒå•æ–‡ä»¶å’Œæ‰¹é‡å¤„ç†

ä½¿ç”¨æ–¹æ³•ï¼š
    # å•æ–‡ä»¶å¤„ç†
    python main.py input.pdf [--output output.pdf] [--api-key YOUR_API_KEY]
    
    # æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹
    python main.py folder_path --batch [--output output_folder]

ç¯å¢ƒå˜é‡ï¼š
    GOOGLE_AI_API_KEY: Google AI APIå¯†é’¥
"""

import argparse
import os
import sys
from pathlib import Path
from tqdm import tqdm
from dotenv import load_dotenv

from pdf_chunker import PDFChunker
from gemini_extractor import GeminiTitleExtractor
from toc_merger import TOCMerger
from pdf_toc_writer import PDFTOCWriter


def process_single_file(args):
    """å¤„ç†å•ä¸ªPDFæ–‡ä»¶"""
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        sys.exit(1)
    
    if not input_path.suffix.lower() == '.pdf':
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸æ˜¯PDFæ ¼å¼: {input_path}")
        sys.exit(1)
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„
    output_path = args.output if args.output else str(input_path)
    
    try:
        print("ğŸš€ Gemini PDF Indexer å¼€å§‹è¿è¡Œ")
        print(f"ğŸ“„ è¾“å…¥æ–‡ä»¶: {input_path}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print()
        
        # æ­¥éª¤1: åˆ†æPDFæ–‡ä»¶
        print("ğŸ“Š åˆ†æPDFæ–‡ä»¶...")
        chunker = PDFChunker(max_pages=args.max_pages)
        total_pages, estimated_size = chunker.get_pdf_info(str(input_path))
        
        print(f"   æ€»é¡µæ•°: {total_pages}")
        print(f"   ä¼°ç®—å¤§å°: {estimated_size:,} å­—ç¬¦")
        print(f"   å°†åˆ†ä¸º {(total_pages + args.max_pages - 1) // args.max_pages} ä¸ªå¤„ç†å—")
        print()
        
        # æ­¥éª¤2: åˆ†å—å¤„ç†
        print("ğŸ“¦ åˆ†å—è¯»å–PDF...")
        chunks = chunker.chunk_pdf(str(input_path))
        print(f"   å·²åˆ›å»º {len(chunks)} ä¸ªå¤„ç†å—")
        print()
        
        # æ­¥éª¤3: åˆå§‹åŒ–Geminiæå–å™¨
        print("ğŸ¤– åˆå§‹åŒ–Gemini AI...")
        try:
            extractor = GeminiTitleExtractor(api_key=args.api_key)
            print("   Gemini 2.5 Flash å·²å°±ç»ª")
        except ValueError as e:
            print(f"   é”™è¯¯: {e}")
            print("   è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GOOGLE_AI_API_KEY æˆ–ä½¿ç”¨ --api-key å‚æ•°")
            sys.exit(1)
        print()
        
        # æ­¥éª¤4: æå–æ ‡é¢˜
        print("ğŸ” æå–æ–‡æ¡£æ ‡é¢˜ç»“æ„...")
        all_toc_entries = []
        
        with tqdm(total=len(chunks), desc="å¤„ç†è¿›åº¦") as pbar:
            for i, (chunk_bytes, start_page, end_page) in enumerate(chunks):
                pbar.set_description(f"å¤„ç†ç¬¬ {start_page}-{end_page} é¡µ")
                
                if args.verbose:
                    print(f"\n   å— {i+1}/{len(chunks)}: ç¬¬ {start_page}-{end_page} é¡µ")
                
                # å°è¯•ç›´æ¥å¤„ç†PDFå­—èŠ‚æµï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°æ–‡æœ¬æå–
                try:
                    toc_entries = extractor.extract_titles_from_pdf_bytes(
                        chunk_bytes, start_page, end_page
                    )
                except Exception as e:
                    if args.verbose:
                        print(f"   PDFç›´æ¥å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ–‡æœ¬æå–: {e}")
                    
                    # å›é€€åˆ°æ–‡æœ¬æå–
                    text = chunker.extract_text_from_chunk(chunk_bytes)
                    toc_entries = extractor.extract_titles_from_text(text, start_page, end_page)
                
                all_toc_entries.append(toc_entries)
                
                if args.verbose and toc_entries:
                    print(f"   æå–åˆ° {len(toc_entries)} ä¸ªæ ‡é¢˜")
                
                pbar.update(1)
        
        print()
        
        # æ­¥éª¤5: åˆå¹¶å’Œæ’åº
        print("ğŸ”„ åˆå¹¶å’Œæ’åºç›®å½•...")
        merger = TOCMerger()
        final_toc = merger.merge_toc_entries(all_toc_entries)
        
        if not final_toc:
            print("   è­¦å‘Š: æœªæå–åˆ°ä»»ä½•ç›®å½•æ¡ç›®")
            print("   å¯èƒ½çš„åŸå› :")
            print("   - PDFæ–‡æ¡£æ²¡æœ‰æ˜ç¡®çš„æ ‡é¢˜ç»“æ„")
            print("   - æ–‡æ¡£æ ¼å¼ä¸é€‚åˆè‡ªåŠ¨æå–")
            print("   - éœ€è¦è°ƒæ•´æå–å‚æ•°")
            return
        
        print(f"   åˆå¹¶åå…± {len(final_toc)} ä¸ªç›®å½•æ¡ç›®")
        print()
        
        # æ˜¾ç¤ºç›®å½•é¢„è§ˆ
        merger.print_toc_preview(final_toc)
        
        # ä¿å­˜JSONæ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.save_json:
            merger.save_toc_to_json(final_toc, args.save_json)
        
        # å¦‚æœåªæ˜¯é¢„è§ˆæ¨¡å¼ï¼Œåˆ°æ­¤ç»“æŸ
        if args.preview_only:
            print("\nâœ… é¢„è§ˆæ¨¡å¼å®Œæˆï¼Œæœªä¿®æ”¹PDFæ–‡ä»¶")
            return
        
        # æ­¥éª¤6: å†™å…¥PDFç›®å½•
        print("ğŸ’¾ å†™å…¥PDFç›®å½•...")
        writer = PDFTOCWriter()
        
        # é¢„è§ˆç°æœ‰ç›®å½•
        if args.verbose:
            existing_toc = writer.preview_existing_toc(str(input_path))
            writer.compare_toc(existing_toc, final_toc)
        
        # å†™å…¥ç›®å½•
        result_path = writer.write_toc_to_pdf(
            str(input_path),
            final_toc,
            output_path if output_path != str(input_path) else None,
            backup=not args.no_backup
        )
        
        print()
        print("âœ… ç›®å½•ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result_path}")
        print(f"ğŸ“‹ ç›®å½•æ¡ç›®: {len(final_toc)} ä¸ª")
        print()
        print("ğŸ‰ ç°åœ¨å¯ä»¥åœ¨PDFé˜…è¯»å™¨ä¸­æŸ¥çœ‹è‡ªåŠ¨ç”Ÿæˆçš„ç›®å½•äº†ï¼")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ è¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def process_batch(args):
    """æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹"""
    from batch_processor import BatchPDFProcessor
    
    try:
        print("ğŸš€ æ‰¹é‡PDFå¤„ç†å™¨å¯åŠ¨")
        print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶å¤¹: {args.input}")
        
        if args.output:
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {args.output}")
        else:
            print("ğŸ“ è¾“å‡ºæ–¹å¼: åŸåœ°å¤„ç†ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰")
        
        print()
        
        # åˆ›å»ºå¤„ç†å™¨
        processor = BatchPDFProcessor(
            api_key=args.api_key,
            max_pages=args.max_pages
        )
        
        # å¼€å§‹æ‰¹é‡å¤„ç†
        stats = processor.process_folder(
            input_folder=args.input,
            output_folder=args.output,
            recursive=args.recursive,
            backup=not args.no_backup,
            save_json=bool(args.save_json),
            skip_existing=not args.no_skip,
            delay_between_files=args.delay if hasattr(args, 'delay') else 1.0
        )
        
        print("\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨ Gemini 2.5 Flash è‡ªåŠ¨æå–PDFæ ‡é¢˜å¹¶åˆ›å»ºç›®å½•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  å•æ–‡ä»¶å¤„ç†:
    python main.py document.pdf
    python main.py input.pdf --output output_with_toc.pdf
    python main.py input.pdf --api-key your_google_ai_api_key
    
  æ‰¹é‡å¤„ç†:
    python main.py /path/to/pdf/folder --batch
    python main.py folder --batch --output output_folder --recursive
    python main.py folder --batch --save-json --delay 2
        """
    )
    
    parser.add_argument('input', help='è¾“å…¥PDFæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºPDFæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶å¤¹ï¼ˆé»˜è®¤è¦†ç›–åŸæ–‡ä»¶ï¼‰')
    parser.add_argument('--api-key', help='Google AI APIå¯†é’¥ï¼ˆä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡GOOGLE_AI_API_KEYè®¾ç½®ï¼‰')
    parser.add_argument('--max-pages', type=int, default=1000, help='æ¯ä¸ªå¤„ç†å—çš„æœ€å¤§é¡µæ•°ï¼ˆé»˜è®¤1000ï¼‰')
    parser.add_argument('--no-backup', action='store_true', help='ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶')
    parser.add_argument('--save-json', help='å°†æå–çš„ç›®å½•ä¿å­˜ä¸ºJSONæ–‡ä»¶')
    parser.add_argument('--preview-only', action='store_true', help='ä»…é¢„è§ˆæå–çš„ç›®å½•ï¼Œä¸å†™å…¥PDFï¼ˆä»…å•æ–‡ä»¶æ¨¡å¼ï¼‰')
    parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    # æ‰¹é‡å¤„ç†ç›¸å…³å‚æ•°
    parser.add_argument('--batch', action='store_true', help='æ‰¹é‡å¤„ç†æ¨¡å¼ï¼ˆå¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFï¼‰')
    parser.add_argument('--recursive', '-r', action='store_true', help='é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰')
    parser.add_argument('--no-skip', action='store_true', help='ä¸è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰')
    parser.add_argument('--delay', type=float, default=1.0, help='æ–‡ä»¶é—´å¤„ç†å»¶è¿Ÿç§’æ•°ï¼ˆæ‰¹é‡æ¨¡å¼ï¼Œé»˜è®¤1ç§’ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"é”™è¯¯: è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        sys.exit(1)
    
    # æ ¹æ®è¾“å…¥ç±»å‹å’Œæ‰¹é‡æ¨¡å¼æ ‡å¿—å†³å®šå¤„ç†æ–¹å¼
    if args.batch or input_path.is_dir():
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        if not input_path.is_dir():
            print(f"é”™è¯¯: æ‰¹é‡æ¨¡å¼éœ€è¦æ–‡ä»¶å¤¹è·¯å¾„: {input_path}")
            sys.exit(1)
        process_batch(args)
    else:
        # å•æ–‡ä»¶å¤„ç†æ¨¡å¼
        if input_path.is_dir():
            print(f"é”™è¯¯: æ£€æµ‹åˆ°æ–‡ä»¶å¤¹è·¯å¾„ï¼Œè¯·ä½¿ç”¨ --batch å‚æ•°å¯ç”¨æ‰¹é‡æ¨¡å¼")
            print(f"ç¤ºä¾‹: python main.py {args.input} --batch")
            sys.exit(1)
        process_single_file(args)


if __name__ == "__main__":
    main()