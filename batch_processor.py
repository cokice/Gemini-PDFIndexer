#!/usr/bin/env python3
"""
æ‰¹é‡PDFå¤„ç†å™¨ - å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„å¤šä¸ªPDFæ–‡ä»¶
"""

import os
import time
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm
import json

from pdf_chunker import PDFChunker
from gemini_extractor import GeminiTitleExtractor
from toc_merger import TOCMerger
from pdf_toc_writer import PDFTOCWriter


class BatchPDFProcessor:
    """æ‰¹é‡PDFå¤„ç†å™¨"""
    
    def __init__(self, api_key: Optional[str] = None, max_pages: int = 1000):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            api_key: Google AI APIå¯†é’¥
            max_pages: æ¯ä¸ªå¤„ç†å—çš„æœ€å¤§é¡µæ•°
        """
        self.chunker = PDFChunker(max_pages=max_pages)
        self.extractor = GeminiTitleExtractor(api_key=api_key)
        self.merger = TOCMerger()
        self.writer = PDFTOCWriter()
        self.max_pages = max_pages
        
        # å¤„ç†ç»Ÿè®¡
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_toc_entries': 0,
            'processing_time': 0
        }
        
        # å¤±è´¥æ–‡ä»¶è®°å½•
        self.failed_files = []
    
    def find_pdf_files(self, folder_path: str, recursive: bool = True) -> List[Path]:
        """
        æŸ¥æ‰¾æ–‡ä»¶å¤¹ä¸­çš„PDFæ–‡ä»¶
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            recursive: æ˜¯å¦é€’å½’æŸ¥æ‰¾å­æ–‡ä»¶å¤¹
            
        Returns:
            List[Path]: PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        folder = Path(folder_path)
        if not folder.exists():
            raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        
        if not folder.is_dir():
            raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}")
        
        # æŸ¥æ‰¾PDFæ–‡ä»¶
        if recursive:
            pdf_files = list(folder.rglob("*.pdf"))
        else:
            pdf_files = list(folder.glob("*.pdf"))
        
        # æŒ‰æ–‡ä»¶åæ’åº
        pdf_files.sort(key=lambda x: x.name.lower())
        
        return pdf_files
    
    def process_folder(self, 
                      input_folder: str, 
                      output_folder: Optional[str] = None,
                      recursive: bool = True,
                      backup: bool = True,
                      save_json: bool = False,
                      skip_existing: bool = True,
                      delay_between_files: float = 1.0) -> Dict:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„PDFæ–‡ä»¶
        
        Args:
            input_folder: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆNoneåˆ™åŸåœ°å¤„ç†ï¼‰
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
            save_json: æ˜¯å¦ä¿å­˜JSONæ ¼å¼çš„ç›®å½•
            skip_existing: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
            delay_between_files: æ–‡ä»¶é—´å¤„ç†å»¶è¿Ÿï¼ˆç§’ï¼‰
            
        Returns:
            Dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        start_time = time.time()
        
        # æŸ¥æ‰¾PDFæ–‡ä»¶
        print(f"ğŸ” æ‰«ææ–‡ä»¶å¤¹: {input_folder}")
        pdf_files = self.find_pdf_files(input_folder, recursive)
        
        if not pdf_files:
            print("âŒ æœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return self.stats
        
        print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        if recursive:
            print("   (åŒ…å«å­æ–‡ä»¶å¤¹)")
        print()
        
        # å‡†å¤‡è¾“å‡ºæ–‡ä»¶å¤¹
        if output_folder:
            output_path = Path(output_folder)
            output_path.mkdir(parents=True, exist_ok=True)
        
        # é‡ç½®ç»Ÿè®¡
        self.stats['total_files'] = len(pdf_files)
        self.failed_files = []
        
        # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
        with tqdm(total=len(pdf_files), desc="å¤„ç†è¿›åº¦", unit="æ–‡ä»¶") as pbar:
            for pdf_file in pdf_files:
                try:
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_description(f"å¤„ç†: {pdf_file.name}")
                    
                    # æ£€æŸ¥æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
                    if skip_existing and self._is_already_processed(pdf_file, output_folder):
                        pbar.write(f"â­ï¸  è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {pdf_file.name}")
                        pbar.update(1)
                        continue
                    
                    # å¤„ç†å•ä¸ªæ–‡ä»¶
                    success = self._process_single_file(
                        pdf_file, 
                        output_folder, 
                        backup, 
                        save_json
                    )
                    
                    if success:
                        self.stats['processed_files'] += 1
                        pbar.write(f"âœ… å®Œæˆ: {pdf_file.name}")
                    else:
                        self.stats['failed_files'] += 1
                        self.failed_files.append(str(pdf_file))
                        pbar.write(f"âŒ å¤±è´¥: {pdf_file.name}")
                    
                    # æ–‡ä»¶é—´å»¶è¿Ÿï¼ˆé¿å…APIé™åˆ¶ï¼‰
                    if delay_between_files > 0:
                        time.sleep(delay_between_files)
                
                except KeyboardInterrupt:
                    pbar.write("\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
                    break
                except Exception as e:
                    self.stats['failed_files'] += 1
                    self.failed_files.append(str(pdf_file))
                    pbar.write(f"âŒ å¤„ç† {pdf_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                
                pbar.update(1)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        self.stats['processing_time'] = time.time() - start_time
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        self._print_summary()
        
        return self.stats
    
    def _process_single_file(self, 
                           pdf_file: Path, 
                           output_folder: Optional[str], 
                           backup: bool, 
                           save_json: bool) -> bool:
        """
        å¤„ç†å•ä¸ªPDFæ–‡ä»¶
        
        Args:
            pdf_file: PDFæ–‡ä»¶è·¯å¾„
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
            backup: æ˜¯å¦å¤‡ä»½
            save_json: æ˜¯å¦ä¿å­˜JSON
            
        Returns:
            bool: æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            # ç¡®å®šè¾“å‡ºè·¯å¾„
            if output_folder:
                output_path = Path(output_folder) / pdf_file.name
            else:
                output_path = pdf_file
            
            # æ­¥éª¤1: åˆ†å—å¤„ç†PDF
            chunks = self.chunker.chunk_pdf(str(pdf_file))
            
            # æ­¥éª¤2: æå–æ ‡é¢˜
            all_toc_entries = []
            
            for chunk_bytes, start_page, end_page in chunks:
                try:
                    toc_entries = self.extractor.extract_titles_from_pdf_bytes(
                        chunk_bytes, start_page, end_page
                    )
                    all_toc_entries.append(toc_entries)
                except Exception as e:
                    print(f"   å— {start_page}-{end_page} å¤„ç†å¤±è´¥: {e}")
                    # ç»§ç»­å¤„ç†å…¶ä»–å—
                    all_toc_entries.append([])
            
            # æ­¥éª¤3: åˆå¹¶ç»“æœ
            final_toc = self.merger.merge_toc_entries(all_toc_entries)
            
            if not final_toc:
                print(f"   è­¦å‘Š: {pdf_file.name} æœªæå–åˆ°ä»»ä½•ç›®å½•æ¡ç›®")
                return False
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_toc_entries'] += len(final_toc)
            
            # æ­¥éª¤4: ä¿å­˜JSONï¼ˆå¦‚æœéœ€è¦ï¼‰
            if save_json:
                json_path = output_path.with_suffix('.json')
                self.merger.save_toc_to_json(final_toc, str(json_path))
            
            # æ­¥éª¤5: å†™å…¥PDFç›®å½•
            self.writer.write_toc_to_pdf(
                str(pdf_file),
                final_toc,
                str(output_path) if output_path != pdf_file else None,
                backup=backup
            )
            
            return True
            
        except Exception as e:
            print(f"   å¤„ç† {pdf_file.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _is_already_processed(self, pdf_file: Path, output_folder: Optional[str]) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆç®€å•æ£€æŸ¥æ˜¯å¦æœ‰å¤‡ä»½æ–‡ä»¶ï¼‰
        
        Args:
            pdf_file: PDFæ–‡ä»¶è·¯å¾„
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹
            
        Returns:
            bool: æ˜¯å¦å·²å¤„ç†
        """
        try:
            if output_folder:
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰å¯¹åº”æ–‡ä»¶
                output_file = Path(output_folder) / pdf_file.name
                return output_file.exists()
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤‡ä»½æ–‡ä»¶
                backup_file = pdf_file.with_name(f"{pdf_file.stem}_backup.pdf")
                return backup_file.exists()
        except:
            return False
    
    def _print_summary(self):
        """æ‰“å°å¤„ç†ç»“æœæ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœæ‘˜è¦")
        print("="*50)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {self.stats['processed_files']}")
        print(f"âŒ å¤„ç†å¤±è´¥: {self.stats['failed_files']}")
        print(f"ğŸ“‹ æ€»ç›®å½•æ¡ç›®: {self.stats['total_toc_entries']}")
        print(f"â±ï¸  æ€»è€—æ—¶: {self.stats['processing_time']:.1f} ç§’")
        
        if self.stats['processed_files'] > 0:
            avg_time = self.stats['processing_time'] / self.stats['processed_files']
            print(f"ğŸ“ˆ å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.1f} ç§’/æ–‡ä»¶")
        
        # æ˜¾ç¤ºå¤±è´¥æ–‡ä»¶
        if self.failed_files:
            print(f"\nâŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
            for failed_file in self.failed_files:
                print(f"   - {failed_file}")
        
        print("="*50)
    
    def save_processing_log(self, log_path: str):
        """
        ä¿å­˜å¤„ç†æ—¥å¿—
        
        Args:
            log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        log_data = {
            'stats': self.stats,
            'failed_files': self.failed_files,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'settings': {
                'max_pages': self.max_pages
            }
        }
        
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ å¤„ç†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_path}")


def main():
    """æ‰¹é‡å¤„ç†å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    from dotenv import load_dotenv
    
    load_dotenv()
    
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„PDFæ–‡ä»¶ï¼Œä¸ºæ¯ä¸ªPDFæ·»åŠ ç›®å½•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
    python batch_processor.py /path/to/pdf/folder
    python batch_processor.py input_folder --output output_folder
    python batch_processor.py folder --recursive --save-json --delay 2
        """
    )
    
    parser.add_argument('input_folder', help='è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆé»˜è®¤åŸåœ°å¤„ç†ï¼‰')
    parser.add_argument('--api-key', help='Google AI APIå¯†é’¥')
    parser.add_argument('--max-pages', type=int, default=1000, help='æ¯ä¸ªå¤„ç†å—çš„æœ€å¤§é¡µæ•°')
    parser.add_argument('--recursive', '-r', action='store_true', help='é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹')
    parser.add_argument('--no-backup', action='store_true', help='ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶')
    parser.add_argument('--save-json', action='store_true', help='ä¿å­˜ç›®å½•ä¸ºJSONæ–‡ä»¶')
    parser.add_argument('--no-skip', action='store_true', help='ä¸è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶')
    parser.add_argument('--delay', type=float, default=1.0, help='æ–‡ä»¶é—´å¤„ç†å»¶è¿Ÿï¼ˆç§’ï¼‰')
    parser.add_argument('--log', help='ä¿å­˜å¤„ç†æ—¥å¿—çš„æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    try:
        print("ğŸš€ æ‰¹é‡PDFå¤„ç†å™¨å¯åŠ¨")
        print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶å¤¹: {args.input_folder}")
        
        if args.output:
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {args.output}")
        else:
            print("ğŸ“ è¾“å‡ºæ–¹å¼: åŸåœ°å¤„ç†ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰")
        
        print()
        
        # åˆ›å»ºå¤„ç†å™¨
        processor = BatchPDFProcessor(
            api_key=args.api_key,
            max_pages=args.max_pages
        )
        
        # å¼€å§‹æ‰¹é‡å¤„ç†
        stats = processor.process_folder(
            input_folder=args.input_folder,
            output_folder=args.output,
            recursive=args.recursive,
            backup=not args.no_backup,
            save_json=args.save_json,
            skip_existing=not args.no_skip,
            delay_between_files=args.delay
        )
        
        # ä¿å­˜æ—¥å¿—
        if args.log:
            processor.save_processing_log(args.log)
        
        print("\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()