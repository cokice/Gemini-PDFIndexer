from typing import List
from pdf_chunker import TOCEntry
import re


class AdvancedTOCMerger:
    """é«˜çº§ç›®å½•åˆå¹¶å’Œå±‚çº§ä¼˜åŒ–å¤„ç†å™¨"""
    
    def __init__(self):
        pass
    
    def merge_toc_entries(self, all_entries: List[List[TOCEntry]]) -> List[TOCEntry]:
        """
        åˆå¹¶å¤šä¸ªå—çš„ç›®å½•æ¡ç›®ï¼Œä½¿ç”¨é«˜çº§ç®—æ³•
        
        Args:
            all_entries: æ‰€æœ‰å—çš„ç›®å½•æ¡ç›®åˆ—è¡¨
            
        Returns:
            List[TOCEntry]: åˆå¹¶åçš„ç›®å½•æ¡ç›®
        """
        merged = []
        
        # å±•å¼€æ‰€æœ‰æ¡ç›®
        for chunk_entries in all_entries:
            merged.extend(chunk_entries)
        
        if not merged:
            return []
        
        # æŒ‰é¡µç æ’åº
        merged.sort(key=lambda x: (x.page, x.level))
        
        # å»é‡å’Œæ¸…ç†
        cleaned = self._remove_duplicates_advanced(merged)
        
        # æ™ºèƒ½å±‚çº§ä¿®æ­£
        validated = self._validate_and_fix_levels_advanced(cleaned)
        
        # æœ€ç»ˆè´¨é‡æ£€æŸ¥
        final_result = self._final_quality_check(validated)
        
        return final_result
    
    def _remove_duplicates_advanced(self, entries: List[TOCEntry]) -> List[TOCEntry]:
        """
        é«˜çº§å»é‡ç®—æ³• - è€ƒè™‘ç›¸ä¼¼æ€§å’Œä¸Šä¸‹æ–‡
        
        Args:
            entries: åŸå§‹ç›®å½•æ¡ç›®åˆ—è¡¨
            
        Returns:
            List[TOCEntry]: å»é‡åçš„ç›®å½•æ¡ç›®
        """
        if not entries:
            return []
        
        unique_entries = []
        
        for current in entries:
            is_duplicate = False
            
            for i, existing in enumerate(unique_entries):
                # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤é¡¹
                if self._is_duplicate_entry(current, existing):
                    # é€‰æ‹©æ›´å¥½çš„ç‰ˆæœ¬
                    better_entry = self._choose_better_entry(current, existing)
                    unique_entries[i] = better_entry
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_entries.append(current)
        
        return unique_entries
    
    def _is_duplicate_entry(self, entry1: TOCEntry, entry2: TOCEntry) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ¡ç›®æ˜¯å¦ä¸ºé‡å¤"""
        # é¡µç ç›¸è¿‘ï¼ˆÂ±2é¡µå†…ï¼‰
        page_close = abs(entry1.page - entry2.page) <= 2
        
        # æ ‡é¢˜ç›¸ä¼¼æ€§æ£€æŸ¥
        title_similar = self._calculate_title_similarity(entry1.title, entry2.title) > 0.8
        
        # å±‚çº§ç›¸åŒæˆ–ç›¸è¿‘
        level_close = abs(entry1.level - entry2.level) <= 1
        
        return page_close and title_similar and level_close
    
    def _calculate_title_similarity(self, title1: str, title2: str) -> float:
        """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼æ€§ï¼ˆ0-1ï¼‰"""
        # æ ‡å‡†åŒ–æ ‡é¢˜
        t1 = self._normalize_title(title1)
        t2 = self._normalize_title(title2)
        
        if t1 == t2:
            return 1.0
        
        # è®¡ç®—ç¼–è¾‘è·ç¦»ç›¸ä¼¼æ€§
        max_len = max(len(t1), len(t2))
        if max_len == 0:
            return 1.0
        
        # ç®€å•çš„ç›¸ä¼¼æ€§è®¡ç®—
        common_chars = sum(1 for a, b in zip(t1, t2) if a == b)
        return common_chars / max_len
    
    def _normalize_title(self, title: str) -> str:
        """æ ‡å‡†åŒ–æ ‡é¢˜ä»¥ä¾¿æ¯”è¾ƒ"""
        # ç§»é™¤ç¼–å·
        title = re.sub(r'^[\d\.\s\(\)â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+', '', title)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå¤šä½™ç©ºæ ¼
        title = re.sub(r'[^\w\u4e00-\u9fff]', '', title)
        return title.lower().strip()
    
    def _choose_better_entry(self, entry1: TOCEntry, entry2: TOCEntry) -> TOCEntry:
        """åœ¨é‡å¤æ¡ç›®ä¸­é€‰æ‹©æ›´å¥½çš„ç‰ˆæœ¬"""
        # ä¼˜å…ˆé€‰æ‹©æ ‡é¢˜æ›´å®Œæ•´çš„
        if len(entry1.title) > len(entry2.title):
            return entry1
        elif len(entry2.title) > len(entry1.title):
            return entry2
        
        # ä¼˜å…ˆé€‰æ‹©å±‚çº§æ›´åˆç†çš„
        if entry1.level < entry2.level:
            return entry1
        elif entry2.level < entry1.level:
            return entry2
        
        # ä¼˜å…ˆé€‰æ‹©é¡µç æ›´é å‰çš„
        if entry1.page < entry2.page:
            return entry1
        
        return entry2
    
    def _validate_and_fix_levels_advanced(self, entries: List[TOCEntry]) -> List[TOCEntry]:
        """
        é«˜çº§å±‚çº§éªŒè¯å’Œä¿®æ­£ç®—æ³•
        
        Args:
            entries: ç›®å½•æ¡ç›®åˆ—è¡¨
            
        Returns:
            List[TOCEntry]: å±‚çº§ä¿®æ­£åçš„ç›®å½•æ¡ç›®
        """
        if not entries:
            return entries
        
        validated = []
        level_stack = []  # è·Ÿè¸ªå±‚çº§æ ˆ
        
        for entry in entries:
            # åˆ†ææ ‡é¢˜æ ¼å¼ä»¥ç¡®å®šæ­£ç¡®å±‚çº§
            predicted_level = self._predict_level_from_format(entry.title)
            
            # ç»“åˆä¸Šä¸‹æ–‡è°ƒæ•´å±‚çº§
            adjusted_level = self._adjust_level_with_context(
                entry.level, predicted_level, level_stack
            )
            
            # åˆ›å»ºæ–°çš„æ¡ç›®
            new_entry = TOCEntry(
                title=entry.title,
                level=adjusted_level,
                page=entry.page
            )
            
            # æ›´æ–°å±‚çº§æ ˆ
            self._update_level_stack(level_stack, adjusted_level)
            
            validated.append(new_entry)
        
        # åå¤„ç†ï¼šç¡®ä¿å±‚çº§çš„è¿ç»­æ€§
        return self._ensure_level_continuity(validated)
    
    def _predict_level_from_format(self, title: str) -> int:
        """åŸºäºæ ‡é¢˜æ ¼å¼é¢„æµ‹å±‚çº§"""
        title = title.strip()
        
        # ä¸€çº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ', title):
            return 1
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€.]', title):
            return 1
        if re.match(r'^\d+\s*[ã€.](?!\d)', title):
            return 1
        
        # äºŒçº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^\d+\.\d+\s', title):
            return 2
        if re.match(r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)', title):
            return 2
        if re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]', title):
            return 2
        
        # ä¸‰çº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^\d+\.\d+\.\d+\s', title):
            return 3
        if re.match(r'^[a-zA-Z]\)', title):
            return 3
        if re.match(r'^[â…°â…±â…²â…³â…´â…µâ…¶â…·â…¸â…¹]', title):
            return 3
        
        # å››çº§æ ‡é¢˜æ¨¡å¼
        if re.match(r'^\d+\.\d+\.\d+\.\d+\s', title):
            return 4
        if re.match(r'^[a-z]\.\s', title):
            return 4
        
        # é»˜è®¤è¿”å›2çº§
        return 2
    
    def _adjust_level_with_context(self, original_level: int, predicted_level: int, level_stack: List[int]) -> int:
        """ç»“åˆä¸Šä¸‹æ–‡è°ƒæ•´å±‚çº§"""
        # å¦‚æœé¢„æµ‹å±‚çº§å¾ˆæ˜ç¡®ï¼Œä½¿ç”¨é¢„æµ‹å±‚çº§
        if predicted_level > 0:
            base_level = predicted_level
        else:
            base_level = original_level
        
        # ç¡®ä¿å±‚çº§ä¸ä¼šè·³è·ƒå¤ªå¤§
        if level_stack:
            last_level = level_stack[-1]
            if base_level > last_level + 1:
                base_level = last_level + 1
        
        # ç¡®ä¿å±‚çº§è‡³å°‘ä¸º1
        return max(1, base_level)
    
    def _update_level_stack(self, level_stack: List[int], current_level: int):
        """æ›´æ–°å±‚çº§æ ˆ"""
        # ç§»é™¤æ¯”å½“å‰å±‚çº§æ›´æ·±çš„å±‚çº§
        while level_stack and level_stack[-1] >= current_level:
            level_stack.pop()
        
        # æ·»åŠ å½“å‰å±‚çº§
        level_stack.append(current_level)
    
    def _ensure_level_continuity(self, entries: List[TOCEntry]) -> List[TOCEntry]:
        """ç¡®ä¿å±‚çº§çš„è¿ç»­æ€§"""
        if not entries:
            return entries
        
        result = []
        last_level = 0
        
        for entry in entries:
            current_level = entry.level
            
            # ç¡®ä¿å±‚çº§ä¸ä¼šè·³è·ƒå¤ªå¤§
            if current_level > last_level + 1:
                current_level = last_level + 1
            
            # ç¡®ä¿å±‚çº§è‡³å°‘ä¸º1
            current_level = max(1, current_level)
            
            result.append(TOCEntry(
                title=entry.title,
                level=current_level,
                page=entry.page
            ))
            
            last_level = current_level
        
        return result
    
    def _final_quality_check(self, entries: List[TOCEntry]) -> List[TOCEntry]:
        """æœ€ç»ˆè´¨é‡æ£€æŸ¥å’Œè¿‡æ»¤"""
        if not entries:
            return entries
        
        filtered = []
        
        for entry in entries:
            # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯æ ‡é¢˜çš„æ¡ç›®
            if self._is_valid_title(entry.title):
                filtered.append(entry)
        
        return filtered
    
    def _is_valid_title(self, title: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆæ ‡é¢˜ï¼ˆæ›´ä¸¥æ ¼çš„æ ‡å‡†ï¼‰"""
        title = title.strip()
        
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(title) < 3 or len(title) > 50:
            return False
        
        # çº¯æ•°å­—ã€æ—¥æœŸæˆ–ç¬¦å·
        if re.match(r'^[\d\s\.\-_å¹´æœˆæ—¥]+$', title):
            return False
        
        # ä»¥å¥å·ã€æ„Ÿå¹å·ã€é—®å·ç»“å°¾çš„ï¼ˆå¯èƒ½æ˜¯å¥å­è€Œéæ ‡é¢˜ï¼‰
        if re.search(r'[ã€‚ï¼ï¼Ÿ]$', title):
            return False
        
        # åŒ…å«è¿‡å¤šæ ‡ç‚¹ç¬¦å·çš„ï¼ˆå¯èƒ½æ˜¯å¥å­ç‰‡æ®µï¼‰
        punctuation_count = len(re.findall(r'[ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ""''ï¼ˆï¼‰]', title))
        if punctuation_count > 2:
            return False
        
        # æ˜æ˜¾çš„æ’é™¤æ¨¡å¼
        exclude_patterns = [
            r'^å›¾\s*\d+',
            r'^è¡¨\s*\d+',
            r'^Figure\s*\d+',
            r'^Table\s*\d+',
            r'ç¬¬\s*\d+\s*é¡µ',
            r'Page\s*\d+',
            r'^å‚è€ƒæ–‡çŒ®',
            r'^è‡´è°¢',
            r'^é™„å½•[A-Z]?$',
            r'^\d{4}å¹´',  # çº¯å¹´ä»½
            r'å…·ä½“è€Œè¨€',   # å¥å­å¼€å¤´
            r'æ ¹æ®.*',    # å¥å­å¼€å¤´
            r'.*ã€‚.*',    # åŒ…å«å¥å·çš„å¥å­
            r'^[a-z]+\.[a-z]+',  # ç½‘å€ç‰‡æ®µ
            r'åŒè¯­ä¼˜åŠ¿',   # æè¿°æ€§å†…å®¹
            r'è¯­è¨€è€ƒè¯•æ›¿ä»£', # æè¿°æ€§å†…å®¹
            r'è¯¾ç¨‹ç¯å¢ƒ',   # æè¿°æ€§å†…å®¹
            r'èƒ½åŠ›.*çš„.*', # æè¿°æ€§å¥å­æ¨¡å¼
            r'é¡¹ç›®.*å…è®¸', # æè¿°æ€§å¥å­æ¨¡å¼
        ]
        
        for pattern in exclude_patterns:
            if re.search(pattern, title, re.IGNORECASE):
                return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„æ ‡é¢˜æ ¼å¼
        title_patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*[ã€.]',
            r'^\d+\s*[ã€.]',
            r'^\d+\.\d+',
            r'^\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+\)',
            r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',
        ]
        
        # å¦‚æœæœ‰æ˜ç¡®çš„æ ‡é¢˜æ ¼å¼ï¼Œç›´æ¥é€šè¿‡
        for pattern in title_patterns:
            if re.match(pattern, title):
                return True
        
        # å¦åˆ™éœ€è¦æ›´ä¸¥æ ¼çš„æ£€æŸ¥
        # ä¸èƒ½åŒ…å«å¤ªå¤šçš„æè¿°æ€§è¯æ±‡
        descriptive_words = ['èƒ½åŠ›', 'ç¯å¢ƒ', 'è¯¾ç¨‹', 'é¡¹ç›®', 'é™¢æ ¡', 'æˆç»©', 'è¯æ˜', 'å…·å¤‡', 'å…è®¸', 'ä½œä¸º', 'åŒæ—¶', 'å‘å±•', 'çŠ¶å†µ', 'æƒ…å†µ', 'å†…å®¹', 'æ–¹é¢']
        descriptive_count = sum(1 for word in descriptive_words if word in title)
        if descriptive_count > 1:
            return False
        
        return True
    
    def format_for_pymupdf(self, entries: List[TOCEntry]) -> List[List]:
        """
        å°†ç›®å½•æ¡ç›®æ ¼å¼åŒ–ä¸ºPyMuPDFéœ€è¦çš„æ ¼å¼
        
        Args:
            entries: ç›®å½•æ¡ç›®åˆ—è¡¨
            
        Returns:
            List[List]: PyMuPDFæ ¼å¼çš„ç›®å½•åˆ—è¡¨
        """
        toc_list = []
        
        for entry in entries:
            # PyMuPDFçš„ç›®å½•æ ¼å¼ï¼š[level, title, page, zoom_info]
            toc_item = [
                entry.level,
                entry.title,
                entry.page
            ]
            toc_list.append(toc_item)
        
        return toc_list
    
    def print_toc_preview(self, entries: List[TOCEntry]) -> None:
        """
        æ‰“å°ç›®å½•é¢„è§ˆï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            entries: ç›®å½•æ¡ç›®åˆ—è¡¨
        """
        print("\n=== æå–çš„ç›®å½•ç»“æ„ ===")
        
        if not entries:
            print("æœªæ‰¾åˆ°ä»»ä½•ç›®å½•æ¡ç›®")
            return
        
        # æŒ‰å±‚çº§ç»Ÿè®¡
        level_counts = {}
        for entry in entries:
            level_counts[entry.level] = level_counts.get(entry.level, 0) + 1
        
        print(f"å±‚çº§ç»Ÿè®¡: {dict(sorted(level_counts.items()))}")
        print()
        
        for entry in entries:
            indent = "  " * (entry.level - 1)
            level_indicator = "â–º" if entry.level == 1 else "â–ª"
            print(f"{indent}{level_indicator} {entry.title} (ç¬¬{entry.page}é¡µ)")
        
        print(f"\næ€»è®¡ {len(entries)} ä¸ªç›®å½•æ¡ç›®")
    
    def save_toc_to_json(self, entries: List[TOCEntry], output_path: str) -> None:
        """
        å°†ç›®å½•ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰
        
        Args:
            entries: ç›®å½•æ¡ç›®åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import json
        from dataclasses import asdict
        import time
        
        # æ„å»ºè¯¦ç»†çš„å…ƒæ•°æ®
        toc_data = {
            'metadata': {
                'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'total_entries': len(entries),
                'level_distribution': {},
                'page_range': {
                    'first': min(entry.page for entry in entries) if entries else 0,
                    'last': max(entry.page for entry in entries) if entries else 0
                }
            },
            'toc_entries': [asdict(entry) for entry in entries]
        }
        
        # ç»Ÿè®¡å±‚çº§åˆ†å¸ƒ
        for entry in entries:
            level = entry.level
            toc_data['metadata']['level_distribution'][level] = \
                toc_data['metadata']['level_distribution'].get(level, 0) + 1
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(toc_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ å¢å¼ºç›®å½•å·²ä¿å­˜åˆ°: {output_path}")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„ç±»å
class TOCMerger(AdvancedTOCMerger):
    """å‘åå…¼å®¹çš„TOCMergerç±»"""
    pass